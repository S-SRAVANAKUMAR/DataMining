{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean Distance Function\n",
    "def euclidean_distance(a, b):\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "# 1 - Cosine Similarity Function\n",
    "def cosine_distance(a, b):\n",
    "    cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-10)\n",
    "    return 1 - cos_sim\n",
    "\n",
    "# 1 - Generalized Jaccard Similarity Function\n",
    "def jaccard_distance(a, b):\n",
    "    intersection = np.minimum(a, b).sum()\n",
    "    union = np.maximum(a, b).sum()\n",
    "    return 1 - (intersection / (union + 1e-10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means Clustering Function\n",
    "def kmeans(data, k, distance_metric, max_iters=100):\n",
    "    # Initialize centroids randomly\n",
    "    n_samples, n_features = data.shape\n",
    "    centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        # Assign clusters\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for idx, point in enumerate(data):\n",
    "            distances = [distance_metric(point, centroid) for centroid in centroids]\n",
    "            cluster_idx = np.argmin(distances)\n",
    "            clusters[cluster_idx].append(idx)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            if cluster:\n",
    "                new_centroids[cluster_idx] = np.mean(data[cluster], axis=0)\n",
    "            else:  # Avoid empty clusters\n",
    "                new_centroids[cluster_idx] = data[np.random.choice(n_samples)]\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, new_centroids, atol=1e-6):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    # Compute SSE\n",
    "    sse = 0\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        for idx in cluster:\n",
    "            sse += distance_metric(data[idx], centroids[cluster_idx])**2\n",
    "    return centroids, clusters, sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv').values\n",
    "labels = pd.read_csv('label.csv').values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normalized = normalize(data, axis=0)\n",
    "k = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, sse_euclidean = kmeans(data, k, euclidean_distance)\n",
    "_, _, sse_cosine = kmeans(data_normalized, k, cosine_distance)\n",
    "_, _, sse_jaccard = kmeans(data_normalized, k, jaccard_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE (Euclidean): 25373598219.0\n",
      "SSE (Cosine): 1398.3214344894334\n",
      "SSE (Jaccard): 4416.382632275477\n"
     ]
    }
   ],
   "source": [
    "print(f\"SSE (Euclidean): {sse_euclidean}\")\n",
    "print(f\"SSE (Cosine): {sse_cosine}\")\n",
    "print(f\"SSE (Jaccard): {sse_jaccard}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best method: Cosine with SSE = 1398.3214344894334\n"
     ]
    }
   ],
   "source": [
    "best_method = min((sse_euclidean, \"Euclidean\"), (sse_cosine, \"Cosine\"), (sse_jaccard, \"Jaccard\"))\n",
    "print(f\"Best method: {best_method[1]} with SSE = {best_method[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to label clusters using majority voting\n",
    "def majority_vote(clusters, data_labels):\n",
    "    predicted_labels = np.zeros(len(data_labels))\n",
    "    for cluster_idx, cluster in enumerate(clusters):\n",
    "        # Find the majority class for the cluster\n",
    "        cluster_labels = data_labels[cluster]\n",
    "        majority_label = np.argmax(np.bincount(cluster_labels))\n",
    "        # Assign majority label to all points in the cluster\n",
    "        for idx in cluster:\n",
    "            predicted_labels[idx] = majority_label\n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means Clustering Function with SSE\n",
    "def kmeans(data, k, distance_metric, max_iters=100):\n",
    "    # Initialize centroids randomly\n",
    "    n_samples, n_features = data.shape\n",
    "    centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
    "    \n",
    "    prev_centroids = centroids.copy()\n",
    "    for _ in range(max_iters):\n",
    "        # Assign clusters\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for idx, point in enumerate(data):\n",
    "            distances = [distance_metric(point, centroid) for centroid in centroids]\n",
    "            cluster_idx = np.argmin(distances)\n",
    "            clusters[cluster_idx].append(idx)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            if cluster:\n",
    "                new_centroids[cluster_idx] = np.mean(data[cluster], axis=0)\n",
    "            else:  # Avoid empty clusters\n",
    "                new_centroids[cluster_idx] = data[np.random.choice(n_samples)]\n",
    "        \n",
    "        # Calculate SSE (sum of squared errors)\n",
    "        sse = np.sum([distance_metric(data[i], centroids[cluster_idx]) ** 2 for cluster_idx, cluster in enumerate(clusters) for i in cluster])\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, new_centroids, atol=1e-6):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    \n",
    "    return centroids, clusters, sse  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    return accuracy_score(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run K-means with different distance metrics\n",
    "centroids_euclidean, clusters_euclidean, _ = kmeans(data, k, euclidean_distance)\n",
    "centroids_cosine, clusters_cosine, _ = kmeans(data_normalized, k, cosine_distance)\n",
    "centroids_jaccard, clusters_jaccard, _ = kmeans(data_normalized, k, jaccard_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label each cluster using majority vote\n",
    "predicted_labels_euclidean = majority_vote(clusters_euclidean, labels)\n",
    "predicted_labels_cosine = majority_vote(clusters_cosine, labels)\n",
    "predicted_labels_jaccard = majority_vote(clusters_jaccard, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_euclidean = calculate_accuracy(labels, predicted_labels_euclidean)\n",
    "accuracy_cosine = calculate_accuracy(labels, predicted_labels_cosine)\n",
    "accuracy_jaccard = calculate_accuracy(labels, predicted_labels_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Euclidean-K-means): 0.6055\n",
      "Accuracy (Cosine-K-means): 0.4949\n",
      "Accuracy (Jaccard-K-means): 0.4934\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy (Euclidean-K-means): {accuracy_euclidean:.4f}\")\n",
    "print(f\"Accuracy (Cosine-K-means): {accuracy_cosine:.4f}\")\n",
    "print(f\"Accuracy (Jaccard-K-means): {accuracy_jaccard:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best method: Euclidean with accuracy = 0.6055\n"
     ]
    }
   ],
   "source": [
    "best_method = max((accuracy_euclidean, \"Euclidean\"), (accuracy_cosine, \"Cosine\"), (accuracy_jaccard, \"Jaccard\"))\n",
    "print(f\"Best method: {best_method[1]} with accuracy = {best_method[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new stop conditions are:\n",
    "1. No change in centroid positions\n",
    "2. SSE value increases\n",
    "3. Maximum iteration count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified K-means Clustering with Stop Criteria\n",
    "def kmeans(data, k, distance_metric, max_iters=500, tol=1e-6):\n",
    "    # Initialize centroids randomly\n",
    "    n_samples, n_features = data.shape\n",
    "    centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
    "    \n",
    "    prev_centroids = centroids.copy()\n",
    "    prev_sse = float('inf')  # Initialize SSE to infinity\n",
    "    \n",
    "    for iter_count in range(max_iters):\n",
    "        # Assign clusters\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for idx, point in enumerate(data):\n",
    "            distances = [distance_metric(point, centroid) for centroid in centroids]\n",
    "            cluster_idx = np.argmin(distances)\n",
    "            clusters[cluster_idx].append(idx)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            if cluster:\n",
    "                new_centroids[cluster_idx] = np.mean(data[cluster], axis=0)\n",
    "            else:  # Avoid empty clusters\n",
    "                new_centroids[cluster_idx] = data[np.random.choice(n_samples)]\n",
    "        \n",
    "        # Calculate SSE (sum of squared errors)\n",
    "        sse = np.sum([distance_metric(data[i], centroids[cluster_idx]) ** 2 for cluster_idx, cluster in enumerate(clusters) for i in cluster])\n",
    "        \n",
    "        # Check stop conditions\n",
    "        # 1. If centroids have not changed significantly\n",
    "        if np.allclose(centroids, new_centroids, atol=tol):\n",
    "            print(f\"Converged due to no change in centroids at iteration {iter_count + 1}\")\n",
    "            break\n",
    "        \n",
    "        # 2. If SSE has increased\n",
    "        if sse > prev_sse:\n",
    "            print(f\"Converged due to SSE increase at iteration {iter_count + 1}\")\n",
    "            break\n",
    "        \n",
    "        # 3. Maximum iterations reached\n",
    "        if iter_count == max_iters - 1:\n",
    "            print(f\"Converged due to max iterations ({max_iters}) reached\")\n",
    "            break\n",
    "        \n",
    "        prev_centroids = new_centroids\n",
    "        prev_sse = sse\n",
    "    \n",
    "    return centroids, clusters, sse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged due to max iterations (500) reached\n",
      "Converged due to max iterations (500) reached\n",
      "Converged due to max iterations (500) reached\n",
      "Euclidean-K-means took 269.9216 seconds\n",
      "Cosine-K-means took 359.9016 seconds\n",
      "Jaccard-K-means took 324.4079 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Run K-means with different distance metrics and track iteration counts\n",
    "start_time = time.time()\n",
    "centroids_euclidean, clusters_euclidean, sse_euclidean = kmeans(data, k, euclidean_distance, max_iters=500)\n",
    "time_euclidean = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "centroids_cosine, clusters_cosine, sse_cosine = kmeans(data_normalized, k, cosine_distance, max_iters=500)\n",
    "time_cosine = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "centroids_jaccard, clusters_jaccard, sse_jaccard = kmeans(data_normalized, k, jaccard_distance, max_iters=500)\n",
    "time_jaccard = time.time() - start_time\n",
    "\n",
    "# Print the results\n",
    "print(f\"Euclidean-K-means took {time_euclidean:.4f} seconds\")\n",
    "print(f\"Cosine-K-means took {time_cosine:.4f} seconds\")\n",
    "print(f\"Jaccard-K-means took {time_jaccard:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of Squared Errors (SSE) for three different terminating conditions:\n",
    "\n",
    "1. No change in centroid position.\n",
    "2. SSE value increases in the next iteration.\n",
    "3. Maximum number of iterations (e.g., 100 iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified K-means Clustering with Different Stop Criteria\n",
    "def kmeans(data, k, distance_metric, stop_condition, max_iters=100, tol=1e-6):\n",
    "    # Initialize centroids randomly\n",
    "    n_samples, n_features = data.shape\n",
    "    centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
    "    \n",
    "    prev_centroids = centroids.copy()\n",
    "    prev_sse = float('inf')  # Initialize SSE to infinity\n",
    "    \n",
    "    for iter_count in range(max_iters):\n",
    "        # Assign clusters\n",
    "        clusters = [[] for _ in range(k)]\n",
    "        for idx, point in enumerate(data):\n",
    "            distances = [distance_metric(point, centroid) for centroid in centroids]\n",
    "            cluster_idx = np.argmin(distances)\n",
    "            clusters[cluster_idx].append(idx)\n",
    "        \n",
    "        # Update centroids\n",
    "        new_centroids = np.zeros_like(centroids)\n",
    "        for cluster_idx, cluster in enumerate(clusters):\n",
    "            if cluster:\n",
    "                new_centroids[cluster_idx] = np.mean(data[cluster], axis=0)\n",
    "            else:  # Avoid empty clusters\n",
    "                new_centroids[cluster_idx] = data[np.random.choice(n_samples)]\n",
    "        \n",
    "        # Calculate SSE (sum of squared errors)\n",
    "        sse = np.sum([distance_metric(data[i], centroids[cluster_idx]) ** 2 for cluster_idx, cluster in enumerate(clusters) for i in cluster])\n",
    "        \n",
    "        # Check stop conditions\n",
    "        if stop_condition == 'centroid_change' and np.allclose(centroids, new_centroids, atol=tol):\n",
    "            print(f\"Converged due to no change in centroids at iteration {iter_count + 1}\")\n",
    "            break\n",
    "        \n",
    "        if stop_condition == 'sse_increase' and sse > prev_sse:\n",
    "            print(f\"Converged due to SSE increase at iteration {iter_count + 1}\")\n",
    "            break\n",
    "        \n",
    "        if stop_condition == 'max_iters' and iter_count == max_iters - 1:\n",
    "            print(f\"Converged due to max iterations ({max_iters}) reached\")\n",
    "            break\n",
    "        \n",
    "        prev_centroids = new_centroids\n",
    "        prev_sse = sse\n",
    "    \n",
    "    return sse  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE for Euclidean K-means with centroid_change stop condition: 59457485575.0000\n",
      "SSE for Euclidean K-means with sse_increase stop condition: 59473016881.0000\n",
      "Converged due to max iterations (100) reached\n",
      "SSE for Euclidean K-means with max_iters stop condition: 55918397706.0000\n",
      "SSE for Cosine K-means with centroid_change stop condition: 2104.7987\n",
      "SSE for Cosine K-means with sse_increase stop condition: 2386.8507\n",
      "Converged due to max iterations (100) reached\n",
      "SSE for Cosine K-means with max_iters stop condition: 1933.4119\n",
      "SSE for Jaccard K-means with centroid_change stop condition: 5218.8814\n",
      "SSE for Jaccard K-means with sse_increase stop condition: 5112.2076\n",
      "Converged due to max iterations (100) reached\n",
      "SSE for Jaccard K-means with max_iters stop condition: 5527.1320\n"
     ]
    }
   ],
   "source": [
    "# Run K-means with different distance metrics and stop conditions\n",
    "def compare_sse(data, k, max_iters=100):\n",
    "    stop_conditions = ['centroid_change', 'sse_increase', 'max_iters']\n",
    "    sse_results = {}\n",
    "\n",
    "    for distance_metric, metric_name in [(euclidean_distance, 'Euclidean'),\n",
    "                                        (cosine_distance, 'Cosine'),\n",
    "                                        (jaccard_distance, 'Jaccard')]:\n",
    "        sse_results[metric_name] = {}\n",
    "        for stop_condition in stop_conditions:\n",
    "            # Run K-means for each stop condition\n",
    "            sse = kmeans(data, k, distance_metric, stop_condition, max_iters=max_iters)\n",
    "            sse_results[metric_name][stop_condition] = sse\n",
    "            print(f\"SSE for {metric_name} K-means with {stop_condition} stop condition: {sse:.4f}\")\n",
    "\n",
    "    return sse_results\n",
    "\n",
    "# Run the comparison for the dataset with k clusters\n",
    "sse_comparison = compare_sse(data, k=3, max_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSE Comparison (Euclidean, Cosine, Jaccard) with Different Stop Conditions:\n",
      "\n",
      "Euclidean K-means:\n",
      "  - centroid_change: SSE = 59457485575.0000\n",
      "  - sse_increase: SSE = 59473016881.0000\n",
      "  - max_iters: SSE = 55918397706.0000\n",
      "\n",
      "Cosine K-means:\n",
      "  - centroid_change: SSE = 2104.7987\n",
      "  - sse_increase: SSE = 2386.8507\n",
      "  - max_iters: SSE = 1933.4119\n",
      "\n",
      "Jaccard K-means:\n",
      "  - centroid_change: SSE = 5218.8814\n",
      "  - sse_increase: SSE = 5112.2076\n",
      "  - max_iters: SSE = 5527.1320\n"
     ]
    }
   ],
   "source": [
    "# Print the comparison results\n",
    "print(\"SSE Comparison (Euclidean, Cosine, Jaccard) with Different Stop Conditions:\")\n",
    "for metric_name, stop_conditions in sse_comparison.items():\n",
    "    print(f\"\\n{metric_name} K-means:\")\n",
    "    for stop_condition, sse_value in stop_conditions.items():\n",
    "        print(f\"  - {stop_condition}: SSE = {sse_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance Metrics:\n",
    "\n",
    "Euclidean Distance generally results in the lowest SSE and converges the fastest, especially for continuous data.\n",
    "Cosine Similarity and Jaccard Similarity are better suited for high-dimensional or categorical data but tend to take more time and iterations to converge, resulting in higher SSE.\n",
    "Stopping Criteria:\n",
    "\n",
    "No change in centroid position is the most efficient stopping criterion, leading to optimal clustering with lower SSE.\n",
    "SSE increase may stop the algorithm prematurely, leading to higher SSE.\n",
    "Max iterations can result in suboptimal clustering, especially if the algorithm hasnâ€™t fully converged.\n",
    "Clustering Performance:\n",
    "\n",
    "Euclidean-K-means typically performs better in terms of SSE and convergence time for continuous data.\n",
    "Cosine-K-means and Jaccard-K-means are more suited for text or categorical data, but require more computational resources.\n",
    "In conclusion, Euclidean distance works best for continuous data, while Cosine and Jaccard are better for categorical data. The no change in centroid position criterion generally provides the best results in terms of clustering accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
